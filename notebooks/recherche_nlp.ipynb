{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9614297e",
   "metadata": {},
   "source": [
    "# Chatbot NLP avec Ollama\n",
    "Agent conversationnel intelligent avec modèle local.                      \n",
    "Realisé par ABOU-EL KASEM KENZA - Novembre 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e30aa90",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aefc2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "==============================================\n",
    "Installation requise:\n",
    "pip install langchain langchain-community\n",
    "pip install spacy nltk chromadb\n",
    "pip install streamlit python-dotenv ollama\n",
    "python -m spacy download fr_core_news_md\n",
    "\n",
    "Installation Ollama:\n",
    "curl -fsSL https://ollama.com/install.sh | sh  (Linux/Mac)\n",
    "ou téléchargez depuis https://ollama.com (Windows)\n",
    "\n",
    "Puis: ollama pull mistral\n",
    "==============================================\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "import spacy\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "\n",
    "# Télécharger les ressources NLTK\n",
    "try:\n",
    "    nltk.download('vader_lexicon', quiet=True)\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4335628d",
   "metadata": {},
   "source": [
    "## ============================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0226ce",
   "metadata": {},
   "source": [
    "### 1. CLASSE NLP - Traitement du langage naturel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f746e9d",
   "metadata": {},
   "source": [
    "## ============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e525dccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NLPProcessor:\n",
    "    \"\"\"Traite le texte avec analyse d'entités, sentiment et intentions\"\"\"\n",
    "    \n",
    "    def __init__(self, language='fr'):\n",
    "        try:\n",
    "            self.nlp = spacy.load('fr_core_news_md')\n",
    "            print(\" Modèle français chargé\")\n",
    "        except:\n",
    "            print(\" Modèle français non trouvé. Installation de l'anglais...\")\n",
    "            try:\n",
    "                self.nlp = spacy.load('en_core_web_sm')\n",
    "            except:\n",
    "                print(\" Aucun modèle spaCy trouvé. Téléchargez avec:\")\n",
    "                print(\"python -m spacy download fr_core_news_md\")\n",
    "                self.nlp = None\n",
    "        \n",
    "        self.sia = SentimentIntensityAnalyzer()\n",
    "        \n",
    "    def extract_entities(self, text):\n",
    "        \"\"\"Extrait les entités nommées (personnes, lieux, organisations)\"\"\"\n",
    "        if not self.nlp:\n",
    "            return []\n",
    "            \n",
    "        doc = self.nlp(text)\n",
    "        entities = []\n",
    "        for ent in doc.ents:\n",
    "            entities.append({\n",
    "                'text': ent.text,\n",
    "                'label': ent.label_,\n",
    "                'start': ent.start_char,\n",
    "                'end': ent.end_char\n",
    "            })\n",
    "        return entities\n",
    "    \n",
    "    def analyze_sentiment(self, text):\n",
    "        \"\"\"Analyse le sentiment du texte\"\"\"\n",
    "        scores = self.sia.polarity_scores(text)\n",
    "        \n",
    "        if scores['compound'] >= 0.05:\n",
    "            sentiment = 'positif'\n",
    "        elif scores['compound'] <= -0.05:\n",
    "            sentiment = 'négatif'\n",
    "        else:\n",
    "            sentiment = 'neutre'\n",
    "            \n",
    "        return {\n",
    "            'sentiment': sentiment,\n",
    "            'score': scores['compound'],\n",
    "            'details': scores\n",
    "        }\n",
    "    \n",
    "    def classify_intent(self, text):\n",
    "        \"\"\"Classifie l'intention de l'utilisateur\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # Règles simples de classification d'intention\n",
    "        if any(word in text_lower for word in ['bonjour', 'salut', 'hello', 'hi', 'coucou']):\n",
    "            return 'salutation'\n",
    "        elif any(word in text_lower for word in ['au revoir', 'bye', 'à bientôt', 'adieu']):\n",
    "            return 'au_revoir'\n",
    "        elif any(word in text_lower for word in ['?', 'comment', 'pourquoi', 'quoi', 'quel', 'qui', 'où', 'quand']):\n",
    "            return 'question'\n",
    "        elif any(word in text_lower for word in ['aide', 'help', 'assistance', 'aidez-moi']):\n",
    "            return 'aide'\n",
    "        elif any(word in text_lower for word in ['merci', 'thank', 'remercie']):\n",
    "            return 'remerciement'\n",
    "        else:\n",
    "            return 'conversation'\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        \"\"\"Nettoie et prépare le texte\"\"\"\n",
    "        if not self.nlp:\n",
    "            return {\n",
    "                'clean_text': text.lower(),\n",
    "                'original': text,\n",
    "                'tokens': text.split()\n",
    "            }\n",
    "            \n",
    "        doc = self.nlp(text)\n",
    "        \n",
    "        # Lemmatisation et nettoyage\n",
    "        tokens = [token.lemma_.lower() for token in doc \n",
    "                 if not token.is_stop and not token.is_punct]\n",
    "        \n",
    "        return {\n",
    "            'clean_text': ' '.join(tokens),\n",
    "            'original': text,\n",
    "            'tokens': tokens\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c79b7e",
   "metadata": {},
   "source": [
    "## ============================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d01900",
   "metadata": {},
   "source": [
    "### 2. CLASSE CHATBOT - Agent conversationnel avec Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e7d77b",
   "metadata": {},
   "source": [
    "## ============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0d8d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ChatbotAgent:\n",
    "    \"\"\"Agent conversationnel intelligent avec Ollama (modèle local gratuit)\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"mistral\", temperature=0.7, base_url=\"http://localhost:11434\"):\n",
    "        \"\"\"\n",
    "        Initialise le chatbot avec Ollama\n",
    "        \n",
    "        Modèles recommandés:\n",
    "        - mistral: Équilibré, bon en français (7B)\n",
    "        - llama2: Performant, anglais principalement (7B)\n",
    "        - phi: Léger et rapide (2.7B)\n",
    "        - neural-chat: Optimisé pour conversation (7B)\n",
    "        - openchat: Bon pour le dialogue (7B)\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\" Initialisation du modèle {model_name}...\")\n",
    "        \n",
    "        try:\n",
    "            # Initialiser Ollama\n",
    "            self.llm = Ollama(\n",
    "                model=model_name,\n",
    "                temperature=temperature,\n",
    "                base_url=base_url,\n",
    "                # callbacks=[StreamingStdOutCallbackHandler()]  # Pour streaming en temps réel\n",
    "            )\n",
    "            \n",
    "            # Test rapide du modèle\n",
    "            test_response = self.llm.invoke(\"Bonjour\")\n",
    "            print(f\" Modèle {model_name} chargé et fonctionnel\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\" Erreur lors du chargement du modèle: {e}\")\n",
    "            print(\"\\n Vérifications:\")\n",
    "            print(\"1. Ollama est-il installé ? Tapez: ollama --version\")\n",
    "            print(\"2. Le service est-il lancé ? Tapez: ollama serve\")\n",
    "            print(f\"3. Le modèle est-il téléchargé ? Tapez: ollama pull {model_name}\")\n",
    "            print(\"\\n Modèles disponibles: ollama list\")\n",
    "            raise\n",
    "        \n",
    "        # Mémoire conversationnelle\n",
    "        self.memory = ConversationBufferMemory(\n",
    "            return_messages=True,\n",
    "            memory_key=\"chat_history\"\n",
    "        )\n",
    "        \n",
    "        # Processeur NLP\n",
    "        self.nlp_processor = NLPProcessor()\n",
    "        \n",
    "        # Template de prompt optimisé pour modèles locaux\n",
    "        template = \"\"\"Tu es un assistant IA serviable, amical et concis. Tu réponds en français de manière naturelle.\n",
    "\n",
    "Historique de conversation:\n",
    "{chat_history}\n",
    "\n",
    "Utilisateur: {input}\n",
    "Assistant:\"\"\"\n",
    "        \n",
    "        self.prompt = PromptTemplate(\n",
    "            input_variables=[\"chat_history\", \"input\"],\n",
    "            template=template\n",
    "        )\n",
    "        \n",
    "        # Chaîne de conversation\n",
    "        self.conversation = ConversationChain(\n",
    "            llm=self.llm,\n",
    "            memory=self.memory,\n",
    "            prompt=self.prompt,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Statistiques\n",
    "        self.stats = {\n",
    "            'total_messages': 0,\n",
    "            'sentiments': {'positif': 0, 'négatif': 0, 'neutre': 0},\n",
    "            'intents': {}\n",
    "        }\n",
    "        \n",
    "        self.model_name = model_name\n",
    "    \n",
    "    def analyze_input(self, user_input):\n",
    "        \"\"\"Analyse complète du message utilisateur\"\"\"\n",
    "        analysis = {\n",
    "            'entities': self.nlp_processor.extract_entities(user_input),\n",
    "            'sentiment': self.nlp_processor.analyze_sentiment(user_input),\n",
    "            'intent': self.nlp_processor.classify_intent(user_input),\n",
    "            'preprocessed': self.nlp_processor.preprocess(user_input)\n",
    "        }\n",
    "        \n",
    "        # Mise à jour des statistiques\n",
    "        self.stats['total_messages'] += 1\n",
    "        sentiment = analysis['sentiment']['sentiment']\n",
    "        self.stats['sentiments'][sentiment] += 1\n",
    "        \n",
    "        intent = analysis['intent']\n",
    "        self.stats['intents'][intent] = self.stats['intents'].get(intent, 0) + 1\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def generate_response(self, user_input, show_analysis=False):\n",
    "        \"\"\"Génère une réponse avec analyse NLP optionnelle\"\"\"\n",
    "        # Analyser l'entrée\n",
    "        analysis = self.analyze_input(user_input)\n",
    "        \n",
    "        # Adapter la réponse selon l'intention\n",
    "        if analysis['intent'] == 'salutation':\n",
    "            context_hint = \"L'utilisateur te salue. Réponds chaleureusement en une phrase.\"\n",
    "        elif analysis['intent'] == 'au_revoir':\n",
    "            context_hint = \"L'utilisateur dit au revoir. Termine poliment en une phrase.\"\n",
    "        elif analysis['intent'] == 'aide':\n",
    "            context_hint = \"L'utilisateur demande de l'aide. Sois clair et utile.\"\n",
    "        else:\n",
    "            context_hint = \"\"\n",
    "        \n",
    "        # Enrichir le prompt avec le contexte NLP\n",
    "        enriched_input = user_input\n",
    "        if context_hint:\n",
    "            enriched_input = f\"{context_hint}\\n{user_input}\"\n",
    "        \n",
    "        try:\n",
    "            # Générer la réponse\n",
    "            response = self.conversation.predict(input=enriched_input)\n",
    "            \n",
    "            # Nettoyer la réponse (enlever les répétitions parfois générées par les modèles locaux)\n",
    "            response = response.strip()\n",
    "            \n",
    "        except Exception as e:\n",
    "            response = f\"Désolé, j'ai rencontré une erreur: {str(e)}\"\n",
    "        \n",
    "        result = {'response': response}\n",
    "        \n",
    "        if show_analysis:\n",
    "            result['analysis'] = analysis\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"Retourne les statistiques de conversation\"\"\"\n",
    "        return self.stats\n",
    "    \n",
    "    def clear_memory(self):\n",
    "        \"\"\"Réinitialise la mémoire de conversation\"\"\"\n",
    "        self.memory.clear()\n",
    "        self.stats = {\n",
    "            'total_messages': 0,\n",
    "            'sentiments': {'positif': 0, 'négatif': 0, 'neutre': 0},\n",
    "            'intents': {}\n",
    "        }\n",
    "    \n",
    "    def get_model_info(self):\n",
    "        \"\"\"Retourne les informations sur le modèle\"\"\"\n",
    "        return {\n",
    "            'model': self.model_name,\n",
    "            'type': 'Ollama (Local)',\n",
    "            'cost': 'Gratuit',\n",
    "            'privacy': '100% Local'\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fed7ae",
   "metadata": {},
   "source": [
    "## ============================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865b3830",
   "metadata": {},
   "source": [
    "### 3. INTERFACE LIGNE DE COMMANDE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b546a810",
   "metadata": {},
   "source": [
    "## ============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8b433d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_cli_chatbot():\n",
    "    \"\"\"Lance le chatbot en mode console\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\" CHATBOT IA AVEC NLP (Version Ollama - Gratuite)\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\nCommandes spéciales:\")\n",
    "    print(\"  /stats    - Afficher les statistiques\")\n",
    "    print(\"  /analyze  - Activer/désactiver l'analyse NLP\")\n",
    "    print(\"  /clear    - Effacer la mémoire\")\n",
    "    print(\"  /model    - Changer de modèle\")\n",
    "    print(\"  /info     - Informations sur le modèle\")\n",
    "    print(\"  /quit     - Quitter\\n\")\n",
    "    \n",
    "    # Demander quel modèle utiliser\n",
    "    print(\" Modèles Ollama disponibles:\")\n",
    "    print(\"  1. mistral (recommandé, bon en français)\")\n",
    "    print(\"  2. llama2 (performant)\")\n",
    "    print(\"  3. phi (léger et rapide)\")\n",
    "    print(\"  4. neural-chat (optimisé conversation)\")\n",
    "    \n",
    "    model_choice = input(\"\\nChoisir le modèle [1-4] ou appuyez sur Entrée pour mistral: \").strip()\n",
    "    \n",
    "    models = {\n",
    "        '1': 'mistral',\n",
    "        '2': 'llama2',\n",
    "        '3': 'phi',\n",
    "        '4': 'neural-chat',\n",
    "        '': 'mistral'\n",
    "    }\n",
    "    \n",
    "    model_name = models.get(model_choice, 'mistral')\n",
    "    \n",
    "    # Créer l'agent\n",
    "    try:\n",
    "        agent = ChatbotAgent(model_name=model_name)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n Impossible de démarrer le chatbot.\")\n",
    "        print(\"\\n Installation rapide:\")\n",
    "        print(\"1. Installez Ollama: https://ollama.com\")\n",
    "        print(f\"2. Téléchargez le modèle: ollama pull {model_name}\")\n",
    "        print(\"3. Relancez ce script\")\n",
    "        return\n",
    "    \n",
    "    show_analysis = False\n",
    "    \n",
    "    print(\"\\n Chatbot prêt ! Commencez à parler...\\n\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            user_input = input(\"Vous: \").strip()\n",
    "            \n",
    "            if not user_input:\n",
    "                continue\n",
    "            \n",
    "            # Commandes spéciales\n",
    "            if user_input.startswith('/'):\n",
    "                if user_input == '/quit':\n",
    "                    print(\"\\n Au revoir !\")\n",
    "                    break\n",
    "                    \n",
    "                elif user_input == '/stats':\n",
    "                    stats = agent.get_stats()\n",
    "                    print(f\"\\n Statistiques:\")\n",
    "                    print(f\"  Messages: {stats['total_messages']}\")\n",
    "                    print(f\"  Sentiments: {stats['sentiments']}\")\n",
    "                    print(f\"  Intentions: {stats['intents']}\\n\")\n",
    "                    continue\n",
    "                    \n",
    "                elif user_input == '/analyze':\n",
    "                    show_analysis = not show_analysis\n",
    "                    status = \"activée\" if show_analysis else \"désactivée\"\n",
    "                    print(f\"\\n Analyse NLP {status}\\n\")\n",
    "                    continue\n",
    "                    \n",
    "                elif user_input == '/clear':\n",
    "                    agent.clear_memory()\n",
    "                    print(\"\\n Mémoire effacée\\n\")\n",
    "                    continue\n",
    "                    \n",
    "                elif user_input == '/info':\n",
    "                    info = agent.get_model_info()\n",
    "                    print(f\"\\n Informations:\")\n",
    "                    print(f\"  Modèle: {info['model']}\")\n",
    "                    print(f\"  Type: {info['type']}\")\n",
    "                    print(f\"  Coût: {info['cost']}\")\n",
    "                    print(f\"  Confidentialité: {info['privacy']}\\n\")\n",
    "                    continue\n",
    "                    \n",
    "                elif user_input == '/model':\n",
    "                    print(\"\\n Pour changer de modèle, relancez le programme\")\n",
    "                    print(\"ou utilisez: ollama run <nom_modèle>\\n\")\n",
    "                    continue\n",
    "            \n",
    "            # Générer la réponse\n",
    "            print(\"\\nRéflexion...\", end=\"\\r\")\n",
    "            result = agent.generate_response(user_input, show_analysis)\n",
    "            print(\" \" * 20, end=\"\\r\")  # Effacer le message\n",
    "            \n",
    "            # Afficher l'analyse si activée\n",
    "            if show_analysis and 'analysis' in result:\n",
    "                analysis = result['analysis']\n",
    "                print(f\"\\n Analyse:\")\n",
    "                print(f\"  Sentiment: {analysis['sentiment']['sentiment']} ({analysis['sentiment']['score']:.2f})\")\n",
    "                print(f\"  Intention: {analysis['intent']}\")\n",
    "                if analysis['entities']:\n",
    "                    print(f\"  Entités: {[e['text'] + ' (' + e['label'] + ')' for e in analysis['entities']]}\")\n",
    "            \n",
    "            # Afficher la réponse\n",
    "            print(f\"\\nBot: {result['response']}\\n\")\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\n Au revoir !\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"\\n Erreur: {e}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e71e7c",
   "metadata": {},
   "source": [
    "## ============================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ea852a",
   "metadata": {},
   "source": [
    "### 4. POINT D'ENTRÉE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5288767e",
   "metadata": {},
   "source": [
    "## ============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4215186c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n Bienvenue ! Ce chatbot utilise Ollama (100% gratuit et local)\")\n",
    "    print(\"Aucune clé API nécessaire - Vos données restent privées\\n\")\n",
    "    \n",
    "    run_cli_chatbot()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
